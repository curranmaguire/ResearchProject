{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory occupied: 920 MB.\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading the data\n",
    "we will store the data in their raw formats using huggingface's datasets library to load the data as it makes porting to NCC easier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "blogs_raw = load_dataset('blog_authorship_corpus', cache_dir=os.getcwd()+ '/Datasets')\n",
    "scientific_raw = load_dataset('scientific_papers', 'arxiv', trust_remote_code=True, cache_dir=os.getcwd() + '/Datasets')\n",
    "journalistic_raw = load_dataset('newsroom', trust_remote_code=True, data_dir= 'Datasets/newsroom/release/')\n",
    "narrative_raw = load_dataset('roneneldan/tinystories', cache_dir=os.getcwd()+ '/Datasets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### exploring the data\n",
    "we will make a datasets list that we can itterate through and then the plan is to work with this to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head of blogs\n",
      "Yeah, so today was ok, late arrival. I'm not in the mood to write much, so..I probably will end up writing a ton. I always end up doing that when I say I don't want to write much. Nothing interesting happened in any of my classes, and I only talked with a couple of good people including, \"You know who\"...except, you don't know who, \"You know who,\" is. Oh well, no one reads this thing anyway. I made it to the top of the rock climbing wall today. Kelly and I intend to conquer the whole wall. It'll be fun. Jazz was good fun as always...heh, and that's really it. Have a lovely evening everyone, and I am off to sleepyland. (I actually didn't write a lot. I'm shocked.)  *Ya di Amore*\n",
      "\n",
      "\n",
      "Yeah, so today was ok, late arrival. I'm not in the mood to write much, so..I probably will end up writing a ton. I always end up doing that when I say I don't want to write much. Nothing interesting happened in any of my classes, and I only talked with a couple of good people including, \"You know who\"...except, you don't know who, \"You know who,\" is. Oh well, no one reads this thing anyway. I made it to the top of the rock climbing wall today. Kelly and I intend to conquer the whole wall. It'll be fun. Jazz was good fun as always...heh, and that's really it. Have a lovely evening everyone, and I am off to sleepyland. (I actually didn't write a lot. I'm shocked.)  *Ya di Amore*\n",
      "\n",
      "\n",
      "Yeah, sorry for not writing for a whole there, but I've had a pretty busy weekend so far. I found out I have about 20 tests on Tuesday because my teachers are evil...but oh well. We had marching yesterday. It was cool. We stayed inside the whole time, and I got to play second part, so the music part will be really easy. I left early from marching at around 5:00, then went out to dinner with my friends (I made the plans before I knew about marching). So, dinner was fun. It was Kelly's birthday, so it was cool. Then we went to Kel's for a sleepover, and did bunches of neat stuff, and I went to sleep earlier than usual for a sleepover since I had to be at a Bar Mitzvah this morning. It was pretty neat. I have to do this project for CR, and we have to go to two different places of worship than our own. So, one is obviously a Jewish synagogue, and I think I'm going to go to a mosque for my other one, but who knows...so, this has been my only chance to write, and I actually have to go now, so I'll write more later...maybe.  *Ya di amore*\n",
      "\n",
      "\n",
      "RAR!\n",
      "\n",
      "\n",
      "Yay, Tuesday...no longer Monday! Whoopie! Plus...late arrival tomorrow which is always good...except, we've only had it once, so yeah...I'll shut up now. Anyway, my day was ok. I sat alone on the bus again! Yay, go being alone on the bus! Umm, I didn't get to finish my EPVM homework before school because Mr. A wouldn't let us in. What a poopeyhead. Anyway, yeah, I ended up just reading  There Are No Children Here . Great book, I'd suggest reading it. Depressing though. So, when we finally got in there, I was like, \"All right! I still have time to do homework,\" but then Nathan's like, \"We're still having sectionals.\" Now you see, we have four people in our trumpet section for jazz because one quit, and one of the kids in the section didn't show up, so our sectional consisted of Nathan, Tom, and I just playing randomly. Oh fun! Nathan kind of forgot to give Alstadt his keys back after opening one of the practice rooms, and took them to math with him. I had to go find him, and get the keys from him. His math teacher kind of glared at me, but oh well. At least I missed warming up in WE. So, yeah, I tried doing the homework in WE, but of course there was Dan, Dave, Peter, and Don being mean...shock shock. At least Matt's a nice kid. So, we played this song in WE (Wow, we played music in music class, woo!) and there was this one part that only 3rd and 4th trumpet play, and I was trying to play it right, but Dan was playing it wrong, and he was sitting next to me, so it's hard not to go with what he's doing.. So...we probably both messed up, and it just didn't combine well, but of course...I was the one who got yelled at for it. Oh well. I'll seek revenge on them someday. We talked about transposing in Music Theory, fun stuff, and they actually said that I'm too hard on myself. I thnk I'm not hard enough on myself. Moving along...EPVM (Nap time!) we talked about past tense, and whatnot. Very boring stuff. Umm, gym...heh, whoops, forgot gym shoes, oh well, it happens. So, I read all period. Math...ok, I was determined not to answer. I don't like answering in that class...but, most of my class...arg...dumb as rocks. It's really obnoxious. I'm not saying I'm an Einstein, but really people...so, I was finally forced to succumb to the evil math devil, and I answered. My math teacher is...well, to say the least a bit boring. Mr. Madsen...eek. Even the name is boring. Oh well...umm, lunch..watched Kelly beat Brett at chess...It was interesting. Comparative Religions was good as usual, and science was...boring, as usual. Talking to Alex and Joanne now.They're really cool kids. Alex put me in his profile...he's back on my nice list. :) Anyway...jazz after school was interesting, but gosh darnit!! I am SO bad at improv Why? I'm probably making it a lot harder than it should be, but...I'm just not the most outgoing person when it comes to playing my trumpet. I was told by a clarinet player that I have a small head, and you know good trumpet players. They all have really big heads, that of course meaning egos. So...possibly I just should have picked flute. Oh well...we just found out we're supposed to have jazz combo on Tuesday's. I had no idea, and already had plans I couldn't cancel, so...now I feel awful for leaving them. Sorry guys...but yeah, I ate dinner, had a lesson, and now here I am...pondering the meaning of life...no not really, but oh well. Nothing more to write now, possibly more later....possibly... ~Ya di amore~\n",
      "\n",
      "\n",
      "head of sci papers\n",
      " additive models play an important role in semiparametric statistics . \n",
      " this paper gives learning rates for regularized kernel based methods for additive models . \n",
      " these learning rates compare favourably in particular in high dimensions to recent results on optimal learning rates for purely nonparametric regularized kernel based quantile regression using the gaussian radial basis function kernel , provided the assumption of an additive model is valid . \n",
      " additionally , a concrete example is presented to show that a gaussian function depending only on one variable lies in a reproducing kernel hilbert space generated by an additive gaussian kernel , but does not belong to the reproducing kernel hilbert space generated by the multivariate gaussian kernel of the same variance .    * \n",
      " key words and phrases . * additive model , kernel , quantile regression , semiparametric , rate of convergence , support vector machine . \n",
      "\n",
      "\n",
      " the effect of a random phase diffuser on fluctuations of laser light ( scintillations ) is studied . \n",
      " not only spatial but also temporal phase variations introduced by the phase diffuser are analyzed . \n",
      " the explicit dependence of the scintillation index on finite - time phase variations is obtained for long propagation paths . \n",
      " it is shown that for large amplitudes of phase fluctuations , a finite - time effect decreases the ability of phase diffuser to suppress the scintillations . \n",
      "\n",
      "\n",
      " this paper investigates , using prior shape models and the concept of ball scale ( b - scale ) , ways of automatically recognizing objects in 3d images without performing elaborate searches or optimization . \n",
      " that is , the goal is to place the model in a single shot close to the right pose ( position , orientation , and scale ) in a given image so that the model boundaries fall in the close vicinity of object boundaries in the image . \n",
      " this is achieved via the following set of key ideas : ( a ) a semi - automatic way of constructing a multi - object shape model assembly . \n",
      " ( b ) a novel strategy of encoding , via b - scale , the pose relationship between objects in the training images and their intensity patterns captured in b - scale images . ( c ) \n",
      " a hierarchical mechanism of positioning the model , in a one - shot way , in a given image from a knowledge of the learnt pose relationship and the b - scale image of the given image to be segmented . \n",
      " the evaluation results on a set of 20 routine clinical abdominal female and male ct data sets indicate the following : ( 1 ) incorporating a large number of objects improves the recognition accuracy dramatically . \n",
      " ( 2 ) the recognition algorithm can be thought as a hierarchical framework such that quick replacement of the model assembly is defined as coarse recognition and delineation itself is known as finest recognition . \n",
      " ( 3 ) scale yields useful information about the relationship between the model assembly and any given image such that the recognition results in a placement of the model close to the actual pose without doing any elaborate searches or optimization . ( 4 ) effective object recognition can make delineation most accurate . \n",
      "\n",
      "\n",
      " in 84 , 258 ( 2000 ) , mateos conjectured that current reversal in a classical deterministic ratchet is associated with bifurcations from chaotic to periodic regimes . \n",
      " this is based on the comparison of the current and the bifurcation diagram as a function of a given parameter for a periodic asymmetric potential . \n",
      " barbi and salerno , in 62 , 1988 ( 2000 ) , have further investigated this claim and argue that , contrary to mateos claim , current reversals can occur also in the absence of bifurcations . \n",
      " barbi and salerno s studies are based on the dynamics of one particle rather than the statistical mechanics of an ensemble of particles moving in the chaotic system . \n",
      " the behavior of ensembles can be quite different , depending upon their characteristics , which leaves their results open to question . in this paper we present results from studies showing how the current depends on the details of the ensemble \n",
      " used to generate it , as well as conditions for convergent behavior ( that is , independent of the details of the ensemble ) . \n",
      " we are then able to present the converged current as a function of parameters , in the same system as mateos as well as barbi and salerno . \n",
      " we show evidence for current reversal without bifurcation , as well as bifurcation without current reversal . \n",
      " we conjecture that it is appropriate to correlate abrupt changes in the current with bifurcation , rather than current reversals , and show numerical evidence for our claims . \n",
      "\n",
      "\n",
      " this paper investigates , using prior shape models and the concept of ball scale ( b - scale ) , ways of automatically recognizing objects in 3d images without performing elaborate searches or optimization . \n",
      " that is , the goal is to place the model in a single shot close to the right pose ( position , orientation , and scale ) in a given image so that the model boundaries fall in the close vicinity of object boundaries in the image . \n",
      " this is achieved via the following set of key ideas : ( a ) a semi - automatic way of constructing a multi - object shape model assembly . \n",
      " ( b ) a novel strategy of encoding , via b - scale , the pose relationship between objects in the training images and their intensity patterns captured in b - scale images . ( c ) \n",
      " a hierarchical mechanism of positioning the model , in a one - shot way , in a given image from a knowledge of the learnt pose relationship and the b - scale image of the given image to be segmented . \n",
      " the evaluation results on a set of 20 routine clinical abdominal female and male ct data sets indicate the following : ( 1 ) incorporating a large number of objects improves the recognition accuracy dramatically . \n",
      " ( 2 ) the recognition algorithm can be thought as a hierarchical framework such that quick replacement of the model assembly is defined as coarse recognition and delineation itself is known as finest recognition . \n",
      " ( 3 ) scale yields useful information about the relationship between the model assembly and any given image such that the recognition results in a placement of the model close to the actual pose without doing any elaborate searches or optimization . ( 4 ) effective object recognition can make delineation most accurate . \n",
      "\n",
      "\n",
      "head of journal papers\n",
      "HAMBURG, Germany, June 3  As he left the soccer field after a club match in the eastern German city of Halle on March 25, the Nigerian forward Adebowale Ogungbure was spit upon, jeered with racial remarks and mocked with monkey noises. In rebuke, he placed two fingers under his nose to simulate a Hitler mustache and thrust his arm in a Nazi salute.\n",
      "\n",
      "Marc Zoro, right, an Ivory Coast native, was a target of racial slurs from the home fans in Messina, Italy. Adriano, a star with Inter Milan, tried to persuade him to stay on the field.\n",
      "\n",
      "From now until its conclusion on July 9, Jeff Z. Klein and other staff members of The Times and International Herald Tribune will track the world's most popular sporting event.\n",
      "\n",
      "Your guide to the games in Germany: teams, rosters, schedules, statistics, venues and more.\n",
      "\n",
      "In April, the American defender Oguchi Onyewu, playing for his professional club team in Belgium, dismissively gestured toward fans who were making simian chants at him. Then, as he went to throw the ball inbounds, Onyewu said a fan of the opposing team reached over a barrier and punched him in the face.\n",
      "\n",
      "International soccer has been plagued for years by violence among fans, including racial incidents. But FIFA, soccer's Zurich-based world governing body, said there has been a recent surge in discriminatory behavior toward blacks by fans and other players, an escalation that has dovetailed with the signing of more players from Africa and Latin America by elite European clubs.\n",
      "\n",
      "This \"deplorable trend,\" as FIFA has called it, now threatens to embarrass the sport on its grandest stage, the World Cup, which opens June 9 for a monthlong run in 12 cities around Germany. More than 30 billion cumulative television viewers are expected to watch part of the competition and Joseph S. Blatter, FIFA's president, has vowed to crack down on racist behavior during the tournament.\n",
      "\n",
      "Underlining FIFA's concerns, the issue has been included on the agenda at its biannual Congress, scheduled to be held this week in Munich. A campaign against bigotry includes \"Say No to Racism\" stadium banners, television commercials, and team captains making pregame speeches during the quarterfinals of the 32-team tournament.\n",
      "\n",
      "Players, coaches and officials have been threatened with sanctions. But FIFA has said it would not be practical to use the harshest penalties available to punish misbehaving fans  halting matches, holding games in empty stadiums and deducting points that teams receive for victories and ties.\n",
      "\n",
      "Players and antiracism experts said they expected offensive behavior during the tournament, including monkey-like chanting; derisive singing; the hanging of banners that reflect neofascist and racist beliefs; and perhaps the tossing of bananas or banana peels, all familiar occurrences during matches in Spain, Italy, eastern Germany and eastern Europe.\n",
      "\n",
      "\"For us it's quite clear this is a reflection of underlying tensions that exist in European societies,\" said Piara Powar, director of the London-based antiracist soccer organization Kick It Out. He said of Eastern Europe: \"Poverty, unemployment, is a problem. Indigenous people are looking for easy answers to blame. Often newcomers bear the brunt of the blame.\"\n",
      "\n",
      "Yet experts and players also said they believed the racist behavior would be more constrained at the World Cup than it was during play in various domestic leagues around Europe, because of increased security, the international makeup of the crowds, higher ticket prices and a sense that spectators would be generally well behaved on soccer's grandest stage.\n",
      "\n",
      "\"We have to differentiate inside and outside the stadium,\" said Kurt Wachter, project coordinator for the Vienna-based Football Against Racism in Europe, a network of organizations that seeks to fight bigotry and xenophobia in 35 countries.\n",
      "\n",
      "\"Racism is a feature of many football leagues inside and outside Europe,\" said Wachter, who expects most problems to occur outside stadiums where crowds are less controlled. \"We're sure we will see some things we're used to seeing. It won't stop because of the World Cup.\"\n",
      "\n",
      "Particularly worrisome are the possibilities of attacks by extremist groups on spectators and visitors in train stations, bars, restaurants and open areas near the stadiums, Wachter and other experts said. To promote tolerance, he said his organization would organize street soccer matches outside World Cup stadiums.\n",
      "\n",
      "Recent attacks in the eastern Germany city of Potsdam on an Ethiopian-born engineer and in eastern Berlin on a state lawmaker of Turkish descent, along with a government report showing an increase in right-wing violence, have ignited fears that even sporadic hate crimes and other intolerant behavior could mar the World Cup, whose embracing motto is A Time to Make Friends.\n",
      "\n",
      "Far-right extremism is isolated on the fringe of German society, and the German government has intended to confront its Nazi past while preaching openness and tolerance. Germany has one of the world's lowest rates of violent crime. Still, an immigrant group called the Africa Council said it would publish a \"No Go\" guide for nonwhites during the World Cup, particularly for some areas of eastern Berlin and for surrounding towns of the state of Brandenburg.\n",
      "\n",
      "In mid-May, a former government spokesman, Uwe-Karsten Heye, caused a furor when he tried to assist visitors by advising that anyone \"with a different skin color\" avoid visiting small and midsize towns in Brandenburg and elsewhere in eastern Germany, or they \"may not leave with their lives.\"\n",
      "\n",
      "These remarks received blunt criticism from high-ranking German officials. Wolfgang Schäuble, the minister of the interior, said there were no areas in which World Cup visitors should feel threatened, calling Germany \"one of the safest places in the world.\"\n",
      "\n",
      "Angela Merkel, Germany's chancellor, has warned that \"anybody who threatens, attacks or, worse, kills anybody because of the color of his skin or because he comes from another country will face the full force of the law.\"\n",
      "\n",
      "The Bundesliga in Germany is one of the world's top professional soccer leagues, and has not experienced widespread racism. Incidents involving racial abuse of black players are more prevalent in semiprofessional and amateur leagues in eastern Germany. One of the cities playing host to the World Cup, Leipzig, is in the former East Germany. Another, Berlin, was partly in East Germany.\n",
      "\n",
      "After making a Nazi salute, which is illegal in Germany, Ogungbure of Nigeria was investigated by the authorities. But a charge of unconstitutional behavior against him was soon dropped because his gesture had been meant to renounce extremist activity.\n",
      "\n",
      "\"I regret what I did,\" Ogungbure said in a telephone interview from Leipzig. \"I should have walked away. I'm a professional, but I'm a human, too. They don't spit on dogs. Why should they spit on me? I felt like a nobody.\"\n",
      "\n",
      "Gerald Asamoah, a forward on Germany's World Cup team and a native of Ghana, has been recounting an incident in the 1990's when he was pelted with bananas before a club match in Cottbus. \"I'll never forget that,\" he said in a television interview. \"It's like we're not people.\" He has expressed anger and sadness over a banner distributed by a right-wing group that admonished, \"No Gerald, You Are Not Germany.\"\n",
      "\n",
      "Cory Gibbs, an American defender who formerly played professionally in Germany, said there were restaurants and nightclubs in eastern Germany  and even around Hamburg in the west  where he was told \"You're not welcome\" because he was black.\n",
      "\n",
      "\"I think racism is everywhere,\" said Gibbs, who will miss the World Cup because of a knee injury. \"But I feel in Germany racism is a lot more direct.\"\n",
      "\n",
      "Racist behavior at soccer matches is primarily displayed by men and is fueled by several factors, according to experts: alcohol; the perceived \"us versus them\" threat of multiculturalism in societies that were once more ethnically homogenous; the difficult economic transition of eastern European nations since the fall of the Berlin Wall; and crude attempts to unnerve opposing players during bitter, consuming rivalries.\n",
      "\n",
      "Other observers say that the soccer stadium in Europe has become a communal soapbox, one of the few remaining public spaces where spectators can be outrageous and where political correctness does not exist and is even discouraged.\n",
      "\n",
      "\"Nowhere else other than football do people meet someplace and have a stage for shouting things as an anonymous mass,\" said Gerd Dembowski, director of a Berlin-based antiracist organization called Floodlight. \"You can shout things you would never say in your normal life, let out your frustrations.\"\n",
      "\n",
      "Not all the misbehavior can be traced to fans or to Europe. Players and coaches have also been transgressors.\n",
      "\n",
      "Luis Aragonés, Spain's World Cup coach, was fined in 2004 after making racial remarks about the French star Thierry Henry. In March, in the Brazilian league, the defender Antonio Carlos was suspended for 120 days, and 4 additional matches, after an incident in which he shouted \"monkey\" at an opposing player who was black. But it was an incident in Spain on Feb. 25 that galvanized antiracist sentiment and prodded FIFA into taking a tougher stand against bigoted behavior. That match, in Zaragoza, was temporarily halted in the 77th minute by the referee, who threatened to cancel the remaining 13 minutes after Samuel Eto'o, the star forward for Barcelona, was subjected to a chorus of racial taunts. Eto'o threatened to leave the field. His coach and teammates eventually persuaded him to continue, and last month Barcelona won the European Champions Cup.\n",
      "\n",
      "Eto'o has become one of the sport's most outspoken players on the subject of racism. \"I'll continue to play,\" Eto'o, whose national team, Cameroon, did not qualify for the World Cup, said this week through his agent. \"I'm not going to give up and hide and put my head down. I'll score goals against the teams whose fans are making rude noises.\"\n",
      "\n",
      "Under pressure to curb what it acknowledged was an increase in racist incidents, FIFA in late March announced a stricter set of penalties that would apply for club and national team matches. The sanctions would include suspensions of five matches for players and officials who make discriminatory gestures, fines of $16,600 to $25,000 for each offense and two-year stadium bans for offending spectators. It also said teams, which receive 3 points in the standings for a victory, would have 3 points deducted on a first offense by misbehaving players, officials or fans.\n",
      "\n",
      "Blatter, the FIFA president, told reporters that the 3-point deduction for abhorrent fan behavior would apply during the World Cup, then backed away from his comments in April. Blatter declined to comment for this article. And it remains unclear exactly what penalties will be levied against World Cup teams for offensive behavior by fans, coaches and players.\n",
      "\n",
      "Nicolas Maingot, a FIFA spokesman, said World Cup sanctions would be made public later. But in an e-mail response to questions, he said: \"Only racist abuses in the field of play will be punished. For fans, it will be impossible, due to the multinationality of the audience. In other words, it would be impossible to identify from which side would potential racist abusers come.\"\n",
      "\n",
      "Critics counter that spectators are supposed to have their names on their tickets, so identifying offending fans should be relatively easy.\n",
      "\n",
      "Onyewu, the American defender who was punched by an opposing fan in Belgium, said the man was identified through an anonymous tip and was barred from attending matches for two years. He said he did not retaliate because he believed that racist behavior reflected acts of a minority of fans.\n",
      "\n",
      "\"I'm anticipating a more professional environment in Germany because it's the World Cup,\" Onyewu said. Even so, he said, although antiracist efforts could restrict public behavior, \"that's only helping the exterior.\"\n",
      "\n",
      "He added, \"The interior mind thinking, you can't really change that.\"\n",
      "\n",
      "\n",
      "By JAMES RUTENBERG and CORKY SIEMASZKO\n",
      "\n",
      "Wednesday, June 25th 1997, 2:02AM\n",
      "\n",
      "A New Jersey teenager who gave birth in a bathroom during her prom strangled and suffocated her newborn son as her date waited outside and classmates grooved on the dance floor, a prosecutor charged yesterday.\n",
      "\n",
      "New details emerged as prosecutors charged mom Melissa Drexler with murder, saying her baby lived long enough to draw several breaths.\n",
      "\n",
      "Drexler then dumped the body in the trash, returned to the dance floor and later noshed on salad, said Monmouth County prosecutor John Kaye.\n",
      "\n",
      "The baby was found by a maintenance worker sent to mop up the blood in the stall.\n",
      "\n",
      "\"We are certain that the baby was alive after it was born,\" said Kaye, who had delayed charging Drexler until the autopsy on the baby was completed.\n",
      "\n",
      "\"The child has marks on its neck that would indicate exactly what happened.\"\n",
      "\n",
      "Investigators aren't sure whether Drexler smothered the baby or if he suffocated inside the knotted plastic garbage bag in which he was found.\n",
      "\n",
      "Drexler, 18, of Forked River, N.J., faces life in prison if convicted on murder charges. She also is charged with endangering the welfare of a child.\n",
      "\n",
      "Kaye said it was unlikely that prosecutors would seek the death penalty against Drexler, citing her youth and lack of a criminal record.\n",
      "\n",
      "Smirking and wearing a sleeveless black-and-blue denim sundress, matching sandals and bright blue nail polish, Drexler did not speak except to say \"yes\" when questioned by Superior Court Judge John Ricciardi.\n",
      "\n",
      "Drexler avoided looking at her parents, John and Maria Drexler, who mortgaged their home to make her $50,000 bail. Her 20-year-old boyfriend and the presumed father of the child, John Lewis, was not in the Monmouth County courtroom.\n",
      "\n",
      "Drexler's attorney, Steven Secare, entered a not guilty plea for his client.\n",
      "\n",
      "Drexler appeared on the verge of tears as she left with her parents. Secare insisted \"she's not very happy and she's very nervous.\"\n",
      "\n",
      "John Drexler later called the charges \"a shocker.\"\n",
      "\n",
      "Last night, Lewis, 20, paid a visit to the Drexler home.\n",
      "\n",
      "Emerging an hour later, he said Drexler was holding up well, but, \"She's very scared.\"\n",
      "\n",
      "Earlier, Kaye revealed horrifying new details about what allegedly happened June 6 at the Lacey Township High School prom.\n",
      "\n",
      "\"No one, as far as we know, no one knew she was pregnant but her,\" Kaye said.\n",
      "\n",
      "Drexler complained of cramps as she drove with Lewis to the Garden Manor banquet hall in Aberdeen Township, said Kaye.\n",
      "\n",
      "Witnesses described seeing blood on the floor while she was closeted in the bathroom stall and hearing sounds that were not \"the normal things you hear in the bathroom,\" according to Kaye.\n",
      "\n",
      "They said Drexler appeared to be trying to wipe the blood from the floor with her shoe.\n",
      "\n",
      "Drexler apparently cut the umbilical cord with the edge of a sanitary napkin dispenser and put the baby in a garbage bag, which she later placed in a trash can, Kaye said.\n",
      "\n",
      "Before Drexler came out of the stall, she told a friend in the bathroom with her that she would be done shortly. \"Go tell the boys we'll be right out,\" said Drexler, according to Kaye.\n",
      "\n",
      "Later, at the prom, when asked by school officials about the blood in the stall, Drexler \"told them it was an extra heavy menstruation,\" said Kaye.\n",
      "\n",
      "The baby died of \"asphyxia due to manual strangulation and obstruction of the external airway or orifices,\" he said.\n",
      "\n",
      "The county medical examiner's office determined that the baby had breathed by doing a microscopic analysis of sacs in his lungs. They also found air in the intestines of the 6-pound, 6-ounce, 19-inch boy.\n",
      "\n",
      "Meanwhile, Drexler's child lay unclaimed at the county morgue. \"It hasn't been named yet,\" said Secare.\n",
      "\n",
      "\n",
      "With Police Commissioner Bernard Kerik cracking the whip, the NYPD is suddenly responding to crime faster than it has in several years.\n",
      "\n",
      "Just four months ago, the average citywide response time to calls of a \"crime in progress\" was slower than it had been at any point during Mayor Giuliani's two terms in office.\n",
      "\n",
      "But since then, according to police documents obtained by the Daily News, the department's front-line cops seem to be turning things around.\n",
      "\n",
      "The NYPD has recently cut its response time by 10% after the measure peaked at 12 minutes in July through October.\n",
      "\n",
      "The citywide average fell to 10.8 minutes from November to February. The last time the NYPD was hitting that mark was in 1999.\n",
      "\n",
      "The 12-minute high was a full minute slower than the same period in 1999 - clearly a cause for concern when the figure was released last month in the semiannual Mayor's Management Report. Giuliani ordered Kerik to do something about it.\n",
      "\n",
      "Kerik responded by tracking response on a daily basis and holding precinct commanders accountable for increases the same way they are for violent crime.\n",
      "\n",
      "\"I don't think response times are where they should be,\" Kerik said. \"The only way to address it is the way I address everything else - start looking at this stuff daily.\"\n",
      "\n",
      "He added: \"If the front-line supervisors know that we're going to be on top of them, they're going to be pro-active. The sergeant is going to be listening to the radio saying, 'Where's the cop?'\"\n",
      "\n",
      "A precinct-by-precinct review shows widely different average response times:\n",
      "\n",
      "* The busy 75th Precinct in East New York, Brooklyn, posted the city's slowest time, at 15.8 minutes (down from 21.3 minutes); the 43rd Precinct in Soundview, the Bronx, was a close second, at 15 minutes.\n",
      "\n",
      "* The 6th Precinct in Greenwich Village and the 26th Precinct on the upper West Side had the best times, 6.9 minutes.\n",
      "\n",
      "The fastest citywide response time the NYPD has recorded since it began tracking response time was 7.9 minutes in 1993; only 10 out of the city's 76 precincts came in under 8 minutes in the most recent survey.\n",
      "\n",
      "* The city's smallest precinct, the 28th in Harlem, which covers about half a square mile, took an average of 8.4 minutes to respond to a crime in progress; the largest, the 122nd in Staten Island (29 square miles), came in at 13.6 minutes.\n",
      "\n",
      "\"Obscene,\" is how Bob Holden, president of the Juniper Park Civic Association, describes the 14.6-minute response time of the 104th Precinct in Ridgewood, Queens.\n",
      "\n",
      "\"I don't think it's the cops' fault; I think there's been a lack of effort by 1 Police Plaza to solve it,\" Holden said.\n",
      "\n",
      "City Councilman Sheldon Leffler (D-Queens), who is chairman of the Public Safety Committee, said Kerik is treating the issue more seriously than his predecessors did. Leffler said former Police Commissioners William Bratton and Howard Safir preferred to cite crime decreases as their measure of police efficiency.\n",
      "\n",
      "\"The fact that it has gone down by a minute in the last four months is good, but I think the public should realize it still reflects a continuing increase from 7.9 in 1993,\" Leffler said.\n",
      "\n",
      "\"The Giuliani administration has had police strategies in many different areas, but they have never, up until now, given the same effort to reducing the response time to crimes in progress.\"\n",
      "\n",
      "Response time measures the period from when the caller gets the 911 operator on the line to the cops' arrival on the scene.\n",
      "\n",
      "Under Kerik's new directive, response-time spreadsheets are dispatched daily to Police Headquarters for analysis. If a precinct shows a sharp spike, or a particular job stands out as taking a long time, Kerik's aides are on the phone to the local commander finding out what took so long.\n",
      "\n",
      "The key to Deputy Inspector Robert Lucena's success in the 26th Precinct has been tracking the busiest periods for 911 calls there, then making sure cops' meal breaks aren't scheduled at the same time.\n",
      "\n",
      "Inspector James Secreto of the 75th Precinct said cops sometimes forget to notify the dispatcher immediately upon arrival at the scene because they are often jumping out of the car to handle serious situations.\n",
      "\n",
      "\"We're making a more conscious effort to let the dispatcher know when we're on the scene,\" he said, adding that as of last week, the average had been lowered to about 13 minutes.\n",
      "\n",
      "Graphic: POLICE RACING AGAINST THE CLOCK\n",
      "\n",
      "6th Precinct: 6.9 minutes ... West Greenwich Village, including the New York University campus. Covers .79 square miles with a population of about 65,000.\n",
      "\n",
      "26th Precinct: 6.9 minutes ... covers W. 110th to W. 141st St. on the upper West Side. Area is 1 square mile. Population is more than 46,000.\n",
      "\n",
      "7th Precinct: 7 minutes ... second-smallest precinct in the city, covering .62 square miles. Population about 60,000.\n",
      "\n",
      "100th Precinct: 7.5 minutes .... Neighborhoods include Arverne, Belle Harbor, Breezy Point, Broad Channel, Neponsit, Rockaway Park and Roxbury, Queens. Population is about 40,000. Covers 3.57 square miles, with 25 miles of waterfront.\n",
      "\n",
      "5th Precinct: 7.6 minutes ... 2.2 square miles covering Chinatown and part of downtown civic center.\n",
      "\n",
      "75th Precinct: 15.8 minutes ... covers 5.5 square miles; neighborhoods of East New York, Cypress Hills and Starrett City. population is 146,857; residential and commercial community includes eight large housing projects.\n",
      "\n",
      "43rd Precinct: 15 minutes ... covers 4.3 square miles...neighborhoods include Soundview, Parkchester, Unionport....population is more than 170,000; private homes, apartment buildings and 20 housing projects.\n",
      "\n",
      "104th Precinct: 14.6 minutes ... covers 7.4 square miles; Ridgewood, Glendale, Middle Village, Maspeth; mostly private homes and commercial buildings. Cops have long complained that several large cemeteries in the middle of the precinct cause delays because cops can't drive through them.\n",
      "\n",
      "47th Precinct: 14.1 minutes ... population 130,000; includes Woodlawn, Wakefield, Williamsbridge, Baychester, Edenwald, Olinville and Fish Bay in the north Bronx. Covers 5.5 square miles, with 88 miles of roadway.\n",
      "\n",
      "114th Precinct: 13.8 minutes ... neighborhoods include Astoria, Long Island City, Woodside and Jackson Heights. Covers 6 square miles. Population about 197,000.\n",
      "\n",
      "Graphic: RESPONSE TIMES BY PRECINCT\n",
      "\n",
      "The gray tones on the map show precincts where police took more than 10 minutes to respond to crimes in progress from November through February. Precincts shown in white had response times of less than 10 minutes. A 10-minute response time is considered average by many big-city police departments across the country.\n",
      "\n",
      "Response times (in minutes) to crimes in progress:\n",
      "\n",
      "1st: Tribeca, Wall St. 11.4 13.4\n",
      "\n",
      "6th: Greenwich Village 7.3 6.9\n",
      "\n",
      "7th: Lower East Side 6.5 7.0\n",
      "\n",
      "9th: Alphabet City 7.2 7.9\n",
      "\n",
      "13th: Gramercy Park 10.8 9.4\n",
      "\n",
      "19th: Upper East Side 10.7 11.3\n",
      "\n",
      "20th: Upper West Side 9.0 8.8\n",
      "\n",
      "23rd: East Harlem 7.3 8.0\n",
      "\n",
      "24th: Upper West Side 8.3 8.0\n",
      "\n",
      "25th: East Harlem 8.8 8.5\n",
      "\n",
      "26th: Morningside Hts. 7.2 6.9\n",
      "\n",
      "28th: Central Harlem 7.6 8.4\n",
      "\n",
      "33rd: Washington Hts. 9.2 7.5\n",
      "\n",
      "40th: Mott Haven 10.6 10.4\n",
      "\n",
      "41st: South Bronx 9.4 8.2\n",
      "\n",
      "44th: Morris Heights 13.5 11.3\n",
      "\n",
      "46th: University Hts. 12.8 11.4\n",
      "\n",
      "52nd: Bedford Park 13.6 11.8\n",
      "\n",
      "60th: Coney Island 10.2 9.3\n",
      "\n",
      "61st: Brighton Beach 14.0 11.3\n",
      "\n",
      "63rd: Flatlands/Mill Basin 11.4 9.9\n",
      "\n",
      "66th: Borough Park 12.7 11.2\n",
      "\n",
      "67th: East Flatbush 12.4 10.6\n",
      "\n",
      "68th: Bay Ridge 10.0 8.8\n",
      "\n",
      "72nd: Sunset Park 11.4 10.4\n",
      "\n",
      "76th: Carroll Gardens 9.7 8.1\n",
      "\n",
      "78th: Park Slope 9.4 9.1\n",
      "\n",
      "75th: East New York 21.3 15.8\n",
      "\n",
      "77th: Crown Heights 14.2 12.4\n",
      "\n",
      "79th: Bed Stuy 13.1 10.9\n",
      "\n",
      "81st: Bed Stuy 11.1 11.6\n",
      "\n",
      "84th: Brooklyn Hts. 10.1 9.3\n",
      "\n",
      "88th: Forth Greene 10.6 9.6\n",
      "\n",
      "101st: Far Rockaway 8.4 7.6\n",
      "\n",
      "102nd: Richmond Hill 13.9 12.0\n",
      "\n",
      "105th: Queens Village 11.5 10.5\n",
      "\n",
      "106th: Ozone Park 14.0 12.9\n",
      "\n",
      "107th: Fresh Meadows 12.5 11.7\n",
      "\n",
      "113th: Springfield Gdns. 11.1 10.8\n",
      "\n",
      "108th: Long Island City 13.1 11.7\n",
      "\n",
      "112th: Forest Hills 10.1 10.4\n",
      "\n",
      "115th: Jackson Hts. 11.0 10.2\n",
      "\n",
      "120th: St. George 10.8 10.6\n",
      "\n",
      "122nd: New Dorp 14.6 13.6\n",
      "\n",
      "\n",
      "BY CORKY SIEMASZKO DAILY NEWS STAFF WRITER With Emily Gest\n",
      "\n",
      "Thursday, September 6th 2001, 2:23AM\n",
      "\n",
      "Mother Teresa believed she was possessed by the Devil, the archbishop of Calcutta said yesterday. So the revered nun, whom the Vatican hopes to make a saint, underwent an exorcism and afterward \"slept like a baby,\" he said.\n",
      "\n",
      "Archbishop Henry D'Souza's bizarre revelation came as millions yesterday marked the fourth anniversary of Mother Teresa's death.\n",
      "\n",
      "But D'Souza told CNN and The Associated Press in India he truly believed Mother Teresa \"might be under the attack of the evil one.\" The Catholic cleric said he diagnosed the demon in Mother Teresa shortly before she had a fatal heart attack Sept. 5, 1997, and died at age 87.\n",
      "\n",
      "The archbishop said the saintly sister was being treated for heart problems at a Calcutta hospital and that by day she appeared calm.\n",
      "\n",
      "But at night, he said, she became \"extremely agitated\" and tore off the wires to the heart monitors.\n",
      "\n",
      "D'Souza said he suggested an exorcism, and the elderly nun, who had devoted her life to helping the poor and downtrodden, quickly agreed.\n",
      "\n",
      "Some doubted the archbishop's story yesterday. The Rev. Richard McBrien, a theology professor at the University of Notre Dame in Indiana, called the exorcism and the archbishop's explanation for it bizarre.\n",
      "\n",
      "So did Michael Cuneo, author of \"American Exorcism,\" which hits bookstores next week.\n",
      "\n",
      "\"You're not supposed to proceed with an exorcism without exhaustive testing,\" he said.\n",
      "\n",
      "Mother Teresa won a Nobel Prize for her life's work, and Pope John Paul has begun the process of declaring her a saint.\n",
      "\n",
      "D'Souza said the exorcism should not hurt Mother Teresa's chances of achieving sainthood.\n",
      "\n",
      "\"In the hearts of those who knew her, she does not need canonization,\" he told the throngs gathered at the Calcutta headquarters of Mother Teresa's religious order, the Missionary Sisters of Charity. \"She is already a saint to them.\"\n",
      "\n",
      "Mother Teresa's sari-clad sisters feed a half-million families a year, treat 90,000 lepers and school more than 20,000 children in Calcutta.\n",
      "\n",
      "Former Mayor Ed Koch, who met the so-called Saint of the Gutters several times, said nobody had less reason for an exorcism than Mother Teresa.\n",
      "\n",
      "\"It was a waste of time,\" Koch said. \"She was as pure as driven snow, before and after.\"\n",
      "\n",
      "In New York, the nuns run an AIDS hospice in Greenwich Village, a women's shelter in Harlem and the Queen of Peace shelter and soup kitchen in the Bronx. Last night, a memorial Mass was said in Mother Teresa's memory at St. Rita's of Cascia in Mott Haven, the Bronx.\n",
      "\n",
      "\n",
      "WASHINGTON, Dec. 23 - The National Security Agency has traced and analyzed large volumes of telephone and Internet communications flowing into and out of the United States as part of the eavesdropping program that President Bush approved after the Sept. 11, 2001, attacks to hunt for evidence of terrorist activity, according to current and former government officials.\n",
      "\n",
      "The volume of information harvested from telecommunication data and voice networks, without court-approved warrants, is much larger than the White House has acknowledged, the officials said. It was collected by tapping directly into some of the American telecommunication system's main arteries, they said.\n",
      "\n",
      "As part of the program approved by President Bush for domestic surveillance without warrants, the N.S.A. has gained the cooperation of American telecommunications companies to obtain backdoor access to streams of domestic and international communications, the officials said.\n",
      "\n",
      "The government's collection and analysis of phone and Internet traffic have raised questions among some law enforcement and judicial officials familiar with the program. One issue of concern to the Foreign Intelligence Surveillance Court, which has reviewed some separate warrant applications growing out of the N.S.A.'s surveillance program, is whether the court has legal authority over calls outside the United States that happen to pass through American-based telephonic \"switches,\" according to officials familiar with the matter.\n",
      "\n",
      "\"There was a lot of discussion about the switches\" in conversations with the court, a Justice Department official said, referring to the gateways through which much of the communications traffic flows. \"You're talking about access to such a vast amount of communications, and the question was, How do you minimize something that's on a switch that's carrying such large volumes of traffic? The court was very, very concerned about that.\"\n",
      "\n",
      "Since the disclosure last week of the N.S.A.'s domestic surveillance program, President Bush and his senior aides have stressed that his executive order allowing eavesdropping without warrants was limited to the monitoring of international phone and e-mail communications involving people with known links to Al Qaeda.\n",
      "\n",
      "What has not been publicly acknowledged is that N.S.A. technicians, besides actually eavesdropping on specific conversations, have combed through large volumes of phone and Internet traffic in search of patterns that might point to terrorism suspects. Some officials describe the program as a large data-mining operation.\n",
      "\n",
      "The current and former government officials who discussed the program were granted anonymity because it remains classified.\n",
      "\n",
      "Bush administration officials declined to comment on Friday on the technical aspects of the operation and the N.S.A.'s use of broad searches to look for clues on terrorists. Because the program is highly classified, many details of how the N.S.A. is conducting it remain unknown, and members of Congress who have pressed for a full Congressional inquiry say they are eager to learn more about the program's operational details, as well as its legality.\n",
      "\n",
      "Officials in the government and the telecommunications industry who have knowledge of parts of the program say the N.S.A. has sought to analyze communications patterns to glean clues from details like who is calling whom, how long a phone call lasts and what time of day it is made, and the origins and destinations of phone calls and e-mail messages. Calls to and from Afghanistan, for instance, are known to have been of particular interest to the N.S.A. since the Sept. 11 attacks, the officials said.\n",
      "\n",
      "This so-called \"pattern analysis\" on calls within the United States would, in many circumstances, require a court warrant if the government wanted to trace who calls whom.\n",
      "\n",
      "The use of similar data-mining operations by the Bush administration in other contexts has raised strong objections, most notably in connection with the Total Information Awareness system, developed by the Pentagon for tracking terror suspects, and the Department of Homeland Security's Capps program for screening airline passengers. Both programs were ultimately scrapped after public outcries over possible threats to privacy and civil liberties.\n",
      "\n",
      "\n",
      "head of stories\n",
      "Once upon a time, there was a little car named Beep. Beep loved to go fast and play in the sun. Beep was a healthy car because he always had good fuel. Good fuel made Beep happy and strong.\n",
      "\n",
      "One day, Beep was driving in the park when he saw a big tree. The tree had many leaves that were falling. Beep liked how the leaves fall and wanted to play with them. Beep drove under the tree and watched the leaves fall on him. He laughed and beeped his horn.\n",
      "\n",
      "Beep played with the falling leaves all day. When it was time to go home, Beep knew he needed more fuel. He went to the fuel place and got more healthy fuel. Now, Beep was ready to go fast and play again the next day. And Beep lived happily ever after.\n",
      "\n",
      "\n",
      "Once upon a time, there was a little girl named Lily. Lily liked to pretend she was a popular princess. She lived in a big castle with her best friends, a cat and a dog.\n",
      "\n",
      "One day, while playing in the castle, Lily found a big cobweb. The cobweb was in the way of her fun game. She wanted to get rid of it, but she was scared of the spider that lived there.\n",
      "\n",
      "Lily asked her friends, the cat and the dog, to help her. They all worked together to clean the cobweb. The spider was sad, but it found a new home outside. Lily, the cat, and the dog were happy they could play without the cobweb in the way. And they all lived happily ever after.\n",
      "\n",
      "\n",
      "One day, a little fish named Fin was swimming near the shore. He saw a big crab and wanted to be friends. \"Hi, I am Fin. Do you want to play?\" asked the little fish. The crab looked at Fin and said, \"No, I don't want to play. I am cold and I don't feel fine.\"\n",
      "\n",
      "Fin felt sad but wanted to help the crab feel better. He swam away and thought of a plan. He remembered that the sun could make things warm. So, Fin swam to the top of the water and called to the sun, \"Please, sun, help my new friend feel fine and not freeze!\"\n",
      "\n",
      "The sun heard Fin's call and shone its warm light on the shore. The crab started to feel better and not so cold. He saw Fin and said, \"Thank you, little fish, for making me feel fine. I don't feel like I will freeze now. Let's play together!\" And so, Fin and the crab played and became good friends.\n",
      "\n",
      "\n",
      "Once upon a time, in a small town, there was a troubled little girl named Lily. She was always sad because she lost her favorite toy, a triangle. She looked everywhere in her house but could not find it.\n",
      "\n",
      "One sunny day, Lily went to the park to play. She saw a big puddle of water and thought her triangle might be there. She put her hand in the water to soak it and looked for her toy. She felt something at the bottom of the puddle.\n",
      "\n",
      "Lily pulled it out and saw that it was her triangle! She was so happy that she found it. From that day on, Lily was never troubled again. She played with her triangle every day and always kept it close to her. And when she saw puddles, she would smile and remember how she found her toy.\n",
      "\n",
      "\n",
      "One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\n",
      "\n",
      "Lily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\n",
      "\n",
      "Together, they shared the needle and sewed the button on Lily's shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"head of blogs\\nDataset({\\n    features: ['text', 'date', 'gender', 'age', 'horoscope', 'job'],\\n    num_rows: 689793\\n})\\nhead of sci papers\\nDataset({\\n    features: ['article', 'abstract', 'section_names'],\\n    num_rows: 203037\\n})\\nhead of journal papers\\nDataset({\\n    features: ['text', 'summary', 'title', 'url', 'date', 'density_bin', 'coverage_bin', 'compression_bin', 'density', 'coverage', 'compression'],\\n    num_rows: 995041\\n})\\nhead of stories\\nDataset({\\n    features: ['text'],\\n    num_rows: 2119719\\n})\\nstories is the largest. with 2 million entries\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = [('blogs', blogs_raw['train']['text']), ('sci papers', scientific_raw['train']['abstract']), ('journal papers', journalistic_raw['train']['text']), ('stories', narrative_raw['train']['text'])]\n",
    "import numpy as np\n",
    "\n",
    "# Generate random indices\n",
    "num_samples = 5\n",
    "\n",
    "# Select samples by random indices\n",
    "\n",
    "\n",
    "for df in datasets:\n",
    "    print(f'head of {df[0]}')\n",
    "    #print(df[1])\n",
    "    random_indices = np.random.randint(0, len(df[0]), num_samples)\n",
    "    for sample in random_indices:\n",
    "        print(df[1][sample])\n",
    "        print('\\n')\n",
    "\n",
    "    \n",
    "'''head of blogs\n",
    "Dataset({\n",
    "    features: ['text', 'date', 'gender', 'age', 'horoscope', 'job'],\n",
    "    num_rows: 689793\n",
    "})\n",
    "head of sci papers\n",
    "Dataset({\n",
    "    features: ['article', 'abstract', 'section_names'],\n",
    "    num_rows: 203037\n",
    "})\n",
    "head of journal papers\n",
    "Dataset({\n",
    "    features: ['text', 'summary', 'title', 'url', 'date', 'density_bin', 'coverage_bin', 'compression_bin', 'density', 'coverage', 'compression'],\n",
    "    num_rows: 995041\n",
    "})\n",
    "head of stories\n",
    "Dataset({\n",
    "    features: ['text'],\n",
    "    num_rows: 2119719\n",
    "})\n",
    "stories is the largest. with 2 million entries\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looking at the respective data\n",
    "if we look at the data samples above we can see that the blog posts are punctuated a lot more, which is probably due to the nature of it being more social media related data. we may need to remove this but let's see. this is in the form of ... and collocations.\n",
    "\n",
    "the scientific data contains latex so we will need to remove this \n",
    "the scientific data also contains large sections of numbers. we will need to remove these as well as references n ,  g. ; bluem ,  h. ; todd ,  a. m.  m. the new ir and thz fel facility at the fritz haber institute in berlin .\\nspie _ * 2015 * , _ 9512 _ , 95121l paarmann ,  a. ; razdolski ,  i. ; melnikov ,  a. ; gewinner ,  s. ; schllkopf ,  w. ; wolf ,  m. second harmonic generation spectroscopy in the reststrahl band of sic using an infrared free - electron laser .\\nlett . _ * 2015 * , _ 107 _ , 081101 ,  a. ; razdolski ,  i. ; gewinner ,\n",
    "\n",
    "potentially using the abstract may be a better idea.\n",
    "\n",
    "journalistic data contains a lot of facts and numbers, this may have to be dealt with. as well as quotes as well as statistical sections which we need to remove.\n",
    "also uses a lot of name data which isn't really needed and can be tagged as name data potentially\n",
    "\n",
    "short stories are quite simplistic and use language that is childish in nature. we may want to change our data to reflect more serious story writing. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brigam most common for blogs:\n",
      "[(('of', 'the'), 5801), (('in', 'the'), 5745), (('and', 'i'), 4880), (('i', 'was'), 3660), (('to', 'be'), 3587), (('it', 'was'), 3576), (('i', 'have'), 3528), (('to', 'the'), 3397), (('on', 'the'), 3263), (('that', 'i'), 3056)]\n",
      "trigam most common for blogs:\n",
      "[(('a', 'lot', 'of'), 746), (('i', 'have', 'to'), 704), (('i', 'dont', 'know'), 683), (('im', 'going', 'to'), 672), (('i', 'want', 'to'), 608), (('i', 'need', 'to'), 518), (('i', 'think', 'i'), 489), (('one', 'of', 'the'), 479), (('i', 'went', 'to'), 476), (('i', 'have', 'a'), 452)]\n",
      "collocations are [('durwood', 'busse'), ('justine', 'bateman'), ('pav', 'tav'), ('1220pm', 'pols2606'), ('dorje', 'phamo'), ('bona', 'fide'), ('feliz', 'navidad'), ('paya', 'lebar'), ('dag', 'nabbit'), ('french2306', 'textes')]\n",
      "brigam most common for sci papers:\n",
      "[(('of', 'the'), 32974), (('in', 'the'), 16597), (('to', 'the'), 9232), (('for', 'the'), 7135), (('that', 'the'), 6411), (('on', 'the'), 6363), (('and', 'the'), 6148), (('of', 'a'), 5170), (('with', 'the'), 5032), (('can', 'be'), 4472)]\n",
      "trigam most common for sci papers:\n",
      "[(('we', 'show', 'that'), 1395), (('we', 'find', 'that'), 1287), (('as', 'well', 'as'), 1063), (('in', 'this', 'paper'), 961), (('show', 'that', 'the'), 888), (('due', 'to', 'the'), 870), (('the', 'presence', 'of'), 851), (('in', 'terms', 'of'), 782), (('the', 'number', 'of'), 746), (('this', 'paper', 'we'), 745)]\n",
      "collocations are [('10214', '4724'), ('baumslag', 'solitar'), ('cahn', 'hilliard'), ('fitzhugh', 'nagumo'), ('gruppo', 'collegato'), ('kullback', 'leibler'), ('licensing', 'provisions'), ('peaceman', 'rachford'), ('baer', 'brock'), ('beverly', 'sackler')]\n",
      "brigam most common for journal papers:\n",
      "[(('of', 'the'), 32211), (('’', 's'), 31897), (('in', 'the'), 27401), (('to', 'the'), 13077), (('on', 'the'), 11712), (('at', 'the'), 9968), (('for', 'the'), 9770), (('’', 't'), 8790), (('in', 'a'), 8613), (('to', 'be'), 8423)]\n",
      "trigam most common for journal papers:\n",
      "[(('it', '’', 's'), 5226), (('one', 'of', 'the'), 2515), (('don', '’', 't'), 2290), (('that', '’', 's'), 1915), (('a', 'lot', 'of'), 1612), (('’', 's', 'a'), 1420), (('i', '’', 'm'), 1275), (('the', 'united', 'states'), 1227), (('there', '’', 's'), 1215), (('as', 'well', 'as'), 1166)]\n",
      "collocations are [('alks', '5461'), ('caran', 'dache'), ('chichén', 'itzá'), ('dursun', 'aydemiranadolu'), ('inga', 'swenson'), ('macular', 'degeneration'), ('márquez', 'grau'), ('reckitt', 'benckiser'), ('reuterskai', 'pfaffenbach'), ('sigmar', 'polke')]\n",
      "brigam most common for stories:\n",
      "[(('was', 'a'), 9219), (('in', 'the'), 8512), (('it', 'was'), 7540), (('one', 'day'), 7299), (('to', 'the'), 6928), (('there', 'was'), 6812), (('upon', 'a'), 6213), (('a', 'time'), 6189), (('once', 'upon'), 6174), (('time', 'there'), 5810)]\n",
      "trigam most common for stories:\n",
      "[(('there', 'was', 'a'), 6432), (('upon', 'a', 'time'), 6177), (('once', 'upon', 'a'), 6174), (('a', 'time', 'there'), 5808), (('time', 'there', 'was'), 5376), (('was', 'a', 'little'), 3106), (('a', 'little', 'girl'), 2457), (('to', 'play', 'with'), 2049), (('was', 'so', 'happy'), 1831), (('little', 'girl', 'named'), 1797)]\n",
      "collocations are [('â', '«'), ('ho', 'ho'), ('recording', 'device'), ('gon', 'na'), ('wan', 'na'), ('eight', 'nine'), ('officer', 'roy'), ('santa', 'claus'), ('seven', 'eight'), ('woof', 'woof')]\n"
     ]
    }
   ],
   "source": [
    "#let's look at the ngrams and collocations of the data.\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "translator = str.maketrans('','', string.punctuation)\n",
    "for dataset in datasets:\n",
    "    texts = dataset[1][:10000]  # Work with a subset for efficiency\n",
    "    tokenized_texts = [word_tokenize(text.lower().translate(translator)) for text in texts]\n",
    "    # Example for bigrams\n",
    "    bigrams = [list(ngrams(tokens, 2)) for tokens in tokenized_texts]\n",
    "\n",
    "    # Example for trigrams\n",
    "    trigrams = [list(ngrams(tokens, 3)) for tokens in tokenized_texts]\n",
    "    bigram_counts = Counter([bigram for text in bigrams for bigram in text])\n",
    "    print(f'brigam most common for {dataset[0]}:\\n{bigram_counts.most_common(10)}' )  # Print the 10 most common bigrams\n",
    "\n",
    "    # Do the same for trigrams\n",
    "    trigram_counts = Counter([trigram for text in trigrams for trigram in text])\n",
    "    print(f'trigam most common for {dataset[0]}:\\n{trigram_counts.most_common(10)}')  # Print the 10 most common trigrams\n",
    "    \n",
    "    # Combine all tokens into a single list\n",
    "    all_tokens = [token for tokens in tokenized_texts for token in tokens]\n",
    "\n",
    "    # Find collocations\n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(all_tokens)\n",
    "    finder.apply_freq_filter(5)  # Only consider bigrams that occur at least 5 times\n",
    "    collocations = finder.nbest(bigram_measures.pmi, 10)  # Top 10 collocations by PMI\n",
    "    print(f'collocations are {collocations}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results and inference\n",
    "\n",
    "#### Blogs\n",
    "we can see that the blogs all use similar language that is quite casual and uses a lot of personal reference (i)\n",
    "\n",
    "#### scientific\n",
    "we can see that the papers we can see the language is quite formal and has no reference to self so it is non personal and formal. most of the collocations are based around references. we should try to remove these and see if more come along\n",
    "\n",
    "#### journals\n",
    "\n",
    "similar with the data being formal mosstly but special note of mentions of the US in the data. this means the data is mostly speaking about america which could mean the data is americanised. this could change the speach as americans could speak differently to english journals.\n",
    "\n",
    "#### stories\n",
    "once upon a time and forms of this are really common. this makes me worry hte text data is quite chidlish so we may need to change the stories data to a larger dataset. maybe look for a specific writer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess function \n",
    "#remove the latex the is in the documents, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 552260\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 162430\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 97458\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import concatenate_datasets, Dataset\n",
    "min_size = min([len(df) for _, df in datasets])\n",
    "np.random.seed(42)\n",
    "\n",
    "def add_source_label(example, label):\n",
    "    example['source_label'] = label\n",
    "    return example\n",
    "new_dataset = {'text':[], \n",
    "                'label':[]}\n",
    "for i in range(len(datasets)):\n",
    "    random_indices = np.random.randint(0, len(datasets[i][1]), min_size)\n",
    "    new_dataset['text'] +=  datasets[i][1][:min_size]\n",
    "    new_dataset['label'] += list(np.full(min_size, i, dtype=int))\n",
    "    \n",
    "from datasets import DatasetDict\n",
    "\n",
    "combined_dataset = Dataset.from_dict(new_dataset)\n",
    "train_test = combined_dataset.train_test_split(test_size=0.2)  # 80% train, 20% test\n",
    "train_val = train_test['train'].train_test_split(test_size=0.15)  # Split remaining train into 80% train, 20% validation\n",
    "\n",
    "# Assemble everything into a single DatasetDict\n",
    "splits = DatasetDict({\n",
    "    'train': train_val['train'],\n",
    "    'test': train_test['test'],\n",
    "    'validation': train_val['test']\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'urlLink    A picture of the kitchen I work in. That\\'s Dave the sous chef there on the right, and in the distance are Frakes and Nicole, the meat cooks. This is my home away from home.&nbsp; urlLink      urlLink    This is my deranged station partner, Jonathan, posing in what we refer to as our \"office.\" It\\'s actually one of the downstairs walk-ins, but we go in there to talk because it\\'s nice and cold.&nbsp; urlLink     It\\'s an off day for the Sox, so I\\'m off topic.  Work was boring tonight, but I love what I do.  We got in fresh abalone today - it was the first time I\\'ve gotten an opportunity to work with it, and it was pretty neat.  Abalone are gastropod mollusks, and they come in still alive so that when you touch the abductor muscle it moves and ripples underneath your fingers.  They\\'re sort of a bitch to clean, and I kept popping open their shit veins, but it was still exciting to play with something new.  That\\'s the really cool thing about working the fish station - we might not be as busy as the meat cooks during service, but we get way more interesting protein than they do.  I\\'m waiting to get eel in one day.  When those things are alive you have to bash their heads on the table, then nail their heads to the wall and peel their skins off straight down, or so I\\'m told...I\\'ve sliced and cut and boiled a lot of things to death, but I\\'ve never had to bash anything.  And to think that I was a vegetarian for five years. I watched John Kerry\\'s acceptance speech tonight.  It was alright.  He\\'s no Bill Clinton though, as far as speech-making goes, but he\\'s also not an insane, fundamentalist war mongering murderer either.  At least he has that going for him. Best part of the speech was when CNN accidentally aired the microphone of the guy who was in charge of releasing all the balloons and confetti at the end: \"More balloons, I need more balloons to fall! Not enough balloons!  Jesus!  Where\\'s the confetti?  What the hell are you guys doing up there?\"  Priceless. CD of the week:  \"Either/Or\" by Elliott Smith.  Soft, sensitive, indie-type songwriters aren\\'t usually my thing, but I like Elliott Smith.  A lot.  And how soft and mushy can you be if you commit suicide by stabbing yourself in the heart? Yowza.   Sox take on the Twins tomorrow in Minnesota.  I\\'m going to bed.', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(splits['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e68ca76e6543bb99d1220c00153f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/552260 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ecbef3865a48e3a5061977562beaf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/162430 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6954d8cfcf74453f8c7833c96a312482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/97458 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "splits['train'].save_to_disk(os.getcwd() + '/Datasets/combined_data/train')\n",
    "splits['test'].save_to_disk(os.getcwd() + '/Datasets/combined_data/test')\n",
    "splits['validation'].save_to_disk(os.getcwd() + '/Datasets/combined_data/validation')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[0;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistilbert/distilbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremove_latex_markup\u001b[39m(text):\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Remove inline math wrapped in $...$\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m$.*?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m$\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n",
      "File \u001b[0;32m~/myjupyterenv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:767\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    766\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m \u001b[43mget_tokenizer_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[1;32m    769\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/myjupyterenv/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:600\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[1;32m    599\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 600\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    617\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/myjupyterenv/lib/python3.8/site-packages/transformers/utils/hub.py:398\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 398\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    413\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[0;32m~/myjupyterenv/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myjupyterenv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1261\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1261\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1262\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1263\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1265\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1266\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1267\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1268\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1269\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1271\u001b[0m         \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n\u001b[1;32m   1272\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m http_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(HUGGINGFACE_HEADER_X_REPO_COMMIT)\n",
      "File \u001b[0;32m~/myjupyterenv/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myjupyterenv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1667\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1664\u001b[0m headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1667\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1673\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1676\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1678\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/myjupyterenv/lib/python3.8/site-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[0;32m~/myjupyterenv/lib/python3.8/site-packages/huggingface_hub/file_download.py:408\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    407\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[0;32m--> 408\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/myjupyterenv/lib/python3.8/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/myjupyterenv/lib/python3.8/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/myjupyterenv/lib/python3.8/site-packages/huggingface_hub/utils/_http.py:67\u001b[0m, in \u001b[0;36mUniqueRequestIdAdapter.send\u001b[0;34m(self, request, *args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Catch any RequestException to append request id to the error message for debugging.\"\"\"\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     69\u001b[0m     request_id \u001b[38;5;241m=\u001b[39m request\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(X_AMZN_TRACE_ID)\n",
      "File \u001b[0;32m~/myjupyterenv/lib/python3.8/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/myjupyterenv/lib/python3.8/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/myjupyterenv/lib/python3.8/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/myjupyterenv/lib/python3.8/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:1348\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/http/client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 277\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/usr/lib/python3.8/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\", use_fast=True)\n",
    "\n",
    "\n",
    "def remove_latex_markup(text):\n",
    "    # Remove inline math wrapped in $...$\n",
    "    text = re.sub(r'\\$.*?\\$', '', text)\n",
    "    \n",
    "    # Remove display math wrapped in \\[...\\]\n",
    "    text = re.sub(r'\\\\\\[.*?\\\\\\]', '', text)\n",
    "    \n",
    "    # Remove simple LaTeX commands like \\command{arg}\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+\\{.*?\\}', '', text)\n",
    "    \n",
    "    # Remove custom @xmath and @xcite commands from the provided example\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove remaining braces after previous replacements\n",
    "    text = re.sub(r'[\\{\\}\\[\\]]', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Apply text cleaning and normalization steps.\"\"\"\n",
    "    # Normalize excessive punctuation, remove numbers, and LaTeX\n",
    "    text = re.sub(r\"\\.\\.\\.+\", \".\", text)\n",
    "    text = re.sub(r\"\\d+(\\.\\d+)?\", \"\", text)\n",
    "    text = remove_latex_markup(text)\n",
    "    # Further custom cleaning steps can be added here\n",
    "    return text\n",
    "\n",
    "def preprocess_and_tokenize(batch):\n",
    "    \"\"\"Clean and tokenize texts using the BERT tokenizer.\"\"\"\n",
    "    # Apply custom text cleaning\n",
    "    batch[\"text\"] = [clean_text(text) for text in batch[\"text\"]]\n",
    "    tokenized_inputs = tokenizer(batch[\"text\"], padding=True, max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    batch.update(tokenized_inputs)\n",
    "    return batch\n",
    "\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train = splits[\"train\"].map(preprocess_and_tokenize, batched=True)\n",
    "encoded_eval = splits['validation'].map(preprocess_and_tokenize, batched=True)\n",
    "encoded_test = splits[\"test\"].map(preprocess_and_tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "id2label = {0: \"BLOG\", 1: \"SCIENTIFIC\", 2:\"JOURNALISTIC\", 3:\"NARRATIVE\"}\n",
    "label2id = {\"BLOG\": 0, \"SCIENTIFIC\": 1, \"JOURNALISTIC\": 2, \"NARRATIVE\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=4, id2label=id2label, label2id=label2id\n",
    ")\n",
    "print(type(model))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"DistilBert_classifier\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "splits[\"train\"] = splits[\"train\"].map(preprocess_and_tokenize, batched=True)\n",
    "splits[\"validation\"] = splits[\"validation\"].map(preprocess_and_tokenize, batched=True)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_train,\n",
    "    eval_dataset=encoded_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.evaluate()\n",
    "!pip install ray[tune]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = encoded_dataset[\"train\"].shard(index=1, num_shards=10) \n",
    "best_run = trainer.hyperparameter_search(n_trials=10, direction=\"maximize\")\n",
    "for n, v in best_run.hyperparameters.items():\n",
    "    setattr(trainer.args, n, v)\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
