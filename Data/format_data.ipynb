{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'url', 'title', 'wiki_intro', 'generated_intro', 'title_len', 'wiki_intro_len', 'generated_intro_len', 'prompt', 'generated_text', 'prompt_tokens', 'generated_text_tokens'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['article', 'abstract', 'section_names'],\n",
      "        num_rows: 203037\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['article', 'abstract', 'section_names'],\n",
      "        num_rows: 6436\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['article', 'abstract', 'section_names'],\n",
      "        num_rows: 6440\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 44972\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 5622\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['document', 'summary'],\n",
      "        num_rows: 5622\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 2119719\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 21990\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#!pip install torch transformers nltk scikit-learn datasets\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split   \n",
    "import os\n",
    "#blogs_raw = load_dataset('blog_authorship_corpus',cache_dir='/home2/cgmj52/ResearchProject/Data/Datasets' )\n",
    "\n",
    "scientific_raw = load_dataset('scientific_papers', 'arxiv', trust_remote_code=True,cache_dir='/home2/cgmj52/ResearchProject/Data/Datasets' )\n",
    "journalistic_raw = load_dataset('multi_news', trust_remote_code=True , )\n",
    "stories_raw = load_dataset('roneneldan/tinystories', )\n",
    "gpt_dataset = load_dataset(\"aadityaubhat/GPT-wiki-intro\",cache_dir='/home2/cgmj52/ResearchProject/Data/Datasets')\n",
    "#gpt2_data = load_dataset(\"spacerini/gpt2-outputs\", cache_dir='/home2/cgmj52/ResearchProject/Data/Datasets' )\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def preprocess_data(text, sentence_length=20):\n",
    "    # Remove LaTeX commands\n",
    "    text = re.sub(r'\\\\[a-zA-Z]+', '', text)\n",
    "    \n",
    "    # Remove specific LaTeX artifacts like @math1 and \\cite{...}\n",
    "    text = re.sub(r'@math\\d+', '', text)  # Assuming @math1, @math2, etc.\n",
    "    text = re.sub(r'@xmath\\d+', '', text) \n",
    "    text = re.sub(r'\\\\cite\\{[^}]*\\}', '', text)  # Remove \\cite{...}\n",
    "    text = re.sub(r'&nbsp;','', text)\n",
    "    # Normalize ellipses and multiple exclamation/question marks to a single instance\n",
    "    text = re.sub(r'\\.\\.\\.+', 'â€¦', text)\n",
    "    text = re.sub(r'!{2,}', '!', text)\n",
    "    text = re.sub(r'\\?{2,}', '?', text)\n",
    "    text = re.sub(('([^\\s\\w]|_)+'), ' ', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    final_array = []\n",
    "    line = []\n",
    "    for word in word_tokenize(text):\n",
    "        # If adding the next word exceeds the sentence_length, write the current line and start a new one\n",
    "        if len(line) >= sentence_length :\n",
    "            final_array.append(' '.join(line))\n",
    "            line = [word]  # Start a new line with the current word\n",
    "        else:\n",
    "            line.append(word)\n",
    "        \n",
    "            # Write any remaining words in the last line\n",
    "    #final_array.append(' '.join(line))\n",
    "\n",
    "    return final_array\n",
    "\n",
    "def write_text_to_file(dataset, output_file, sentence_length, max_lines=500000):\n",
    "    line_count = 0\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for example in dataset:\n",
    "            for line in example:\n",
    "                if line_count > max_lines:\n",
    "                    print(line_count)\n",
    "                    return line_count\n",
    "                line_count += 1\n",
    "                f.write(line + '\\n')\n",
    "           \n",
    "                \n",
    "        print(line_count)\n",
    "        return line_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500001\n",
      "46032\n",
      "45657\n",
      "500001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500001"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_length = 20\n",
    "gpt = list(map(preprocess_data, gpt_dataset['train']['generated_intro']))\n",
    "\n",
    "train, test, = train_test_split(gpt, test_size=0.1, random_state=42)\n",
    "test, dev = train_test_split(test, test_size=0.5, random_state=42)\n",
    "\n",
    "max_lines = write_text_to_file(train, 'gpt.train', sentence_length )\n",
    "test_lines = write_text_to_file(test, 'gpt.test', sentence_length )\n",
    "dev_lines = write_text_to_file(dev, 'gpt.dev', sentence_length )\n",
    "write_text_to_file(gpt, 'gpt.whole', sentence_length )\n",
    "#gpt2_data = map(preprocess_text, gpt2_data['train']['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scientific = list(map(preprocess_data,scientific_raw['train']['abstract']))\n",
    "train, test, = train_test_split(scientific, test_size=0.3, random_state=42)\n",
    "test, dev = train_test_split(test, test_size=0.5, random_state=42)\n",
    "\n",
    "write_text_to_file(train, 'scientific.train', sentence_length, max_lines-1 )\n",
    "write_text_to_file(test, 'scientific.test', sentence_length, test_lines-1 )\n",
    "write_text_to_file(dev, 'scientific.dev', sentence_length, dev_lines-1 )\n",
    "\n",
    "write_text_to_file(scientific, 'scientific.whole', sentence_length )\n",
    "write_text_to_file(scientific + gpt, 'gpt_sci.corpus', sentence_length, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000001\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000001"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "write_text_to_file(scientific + gpt, 'gpt_sci.corpus', sentence_length, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "journals_train = journalistic_raw['train']['abstract'].map(preprocess_data)\n",
    "journals_test = journalistic_raw['test']['abstract'].map(preprocess_data)\n",
    "journals_val = journalistic_raw['validation']['abstract'].map(preprocess_data)\n",
    "write_text_to_file(journals_train, 'journals.train', sentence_length )\n",
    "write_text_to_file(journals_test, 'journals.test', sentence_length )\n",
    "write_text_to_file(journals_val, 'journals.dev', sentence_length )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "narrative = narrative_raw['train']['text'].map(preprocess_data)\n",
    "train, test, = train_test_split(narrative, test_size=0.3, random_state=42)\n",
    "test, dev = train_test_split(test, test_size=0.5, random_state=42)\n",
    "write_text_to_file(train, 'narrative.train', sentence_length )\n",
    "write_text_to_file(test, 'narrative.test', sentence_length )\n",
    "write_text_to_file(dev, 'narrative.dev', sentence_length )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate samples\n",
    "gpt_sample = gpt_dataset['train']['generated_intro'][:500]\n",
    "stories_sample = stories_raw['train']['text'][:500]\n",
    "journal_sample = journalistic_raw['train']['document'][:500]\n",
    "scientific_sample = scientific_raw['train']['abstract'][:500]\n",
    "\n",
    "#create an itterable so we can process each dataset\n",
    "datasets = [gpt_sample, stories_sample, journal_sample, scientific_sample]\n",
    "#https://www.geeksforgeeks.org/how-to-generate-word-embedding-using-bert/\n",
    "# importing libraries\n",
    "import random\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "# Set a random seed\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    " \n",
    "# Set a random seed for PyTorch (for GPU as well)\n",
    "torch.manual_seed(random_seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(random_seed)\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "word_embedding_datasets = []\n",
    "embeddings_labels = []\n",
    "labels = ['GPT style', 'stories', 'news reports', 'scientific']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "for y, dataset in enumerate(datasets):\n",
    "    encoding = tokenizer.batch_encode_plus(\n",
    "        dataset,                    # List of input texts\n",
    "        padding=True,              # Pad to the maximum sequence length\n",
    "        truncation=True,           # Truncate to the maximum sequence length if necessary\n",
    "        return_tensors='pt',      # Return PyTorch tensors\n",
    "        add_special_tokens=True    # Add special tokens CLS and SEP\n",
    "    )\n",
    " \n",
    "    input_ids = encoding['input_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "    # Generate embeddings using BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        word_embeddings = outputs.last_hidden_state[:,-1,:].numpy()  # This contains the embeddings\n",
    "        word_embedding_datasets.append(word_embeddings)\n",
    "        embeddings_labels += [y]*len(word_embeddings)\n",
    "    print(f'completed dataset....{labels[y]}')\n",
    "\n",
    "#max_length = max(len(seq) for seq in word_embedding_datasets)\n",
    "#word_embedding_datasets = [np.pad(seq, ((0, max_length - len(seq)), (0, 0)), mode='constant') for seq in word_embedding_datasets]\n",
    "word_embedding_datasets = np.concatenate(word_embedding_datasets, axis=0)\n",
    "print(word_embedding_datasets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1536000,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#due to the number of data points and dimensionality. It is best to use the GPU\n",
    "#to install tsne cuda use: (must have python 3.9)\n",
    "##cuda version is 12.4\n",
    "'''!pip install \\\n",
    "    --extra-index-url=https://pypi.nvidia.com \\\n",
    "    cudf-cu12==24.4.* dask-cudf-cu12==24.4.* cuml-cu12==24.4.* \\\n",
    "    cugraph-cu12==24.4.* cuspatial-cu12==24.4.* cuproj-cu12==24.4.* \\\n",
    "    cuxfilter-cu12==24.4.* cucim-cu12==24.4.* pylibraft-cu12==24.4.* \\\n",
    "    raft-dask-cu12==24.4.* cuvs-cu12==24.4.*'''\n",
    "\n",
    "\n",
    "perplexity = np.arange(10,500, 50)\n",
    "divergence = []\n",
    "\n",
    "for i in perplexity:\n",
    "    model = TSNE(n_components=2, perplexity=i)\n",
    "    reduced = model.fit_transform(all_embeddings)\n",
    "    divergence.append(model.kl_divergence_)\n",
    "\n",
    "fig = px.line(x=perplexity, y=divergence, markers=True)\n",
    "fig.update_layout(xaxis_title=\"Perplexity Values\", yaxis_title=\"Divergence\")\n",
    "fig.update_traces(line_color=\"red\", line_width=1)\n",
    "fig.show()\n",
    "\n",
    "#takes around 4 hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the t-SNE results\n",
    "model = TSNE(n_components=2, perplexity=500)\n",
    "tsne_embeddings = model.fit_transform(all_embeddings)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "colors = ['red', 'blue', 'green', 'orange']\n",
    "for i, label in enumerate(labels):\n",
    "    mask = [x == i for x in embeddings_labels]\n",
    "    plt.scatter(tsne_embeddings[mask, 0], tsne_embeddings[mask, 1], c=colors[i], alpha=0.8, label=label)\n",
    "\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.title('t-SNE Plot of Word Embeddings')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams and callocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "flattened_datasets = []\n",
    "for dataset in datasets:\n",
    "    #flatten the preprocessed lists\n",
    "    flattened_dataset = []\n",
    "    for i in range(len(dataset)):\n",
    "        final_sentence =[]\n",
    "        for sentence in dataset[i]:\n",
    "            final_sentence += sentence \n",
    "        flattened_dataset.append(final_sentence)\n",
    "    flattened_datasets.append(flattenend_dataset)\n",
    "\n",
    "for i, dataset in enumerate(flattened_datasets):\n",
    "    tokenized_sentences = [word_tokenize(sentence) for sentence in dataset]\n",
    "    sentence_lengths = [len(sentence) for sentence in tokenized_sentences]\n",
    "    #tokenize and get the sentence lengths\n",
    "    avg_sentence_length = sum(sentence_lengths) / len(sentence_lengths)\n",
    "    print(f\"Average sentence length for {labels[i]}: {avg_sentence_length:.2f} words\")\n",
    "    # Flatten the tokenized sentences into a single list of words\n",
    "    words = [word for sentence in tokenized_sentences for word in sentence]\n",
    "\n",
    "    # Create a BigramCollocationFinder object\n",
    "    finder = BigramCollocationFinder.from_words(words)\n",
    "\n",
    "    # Apply frequency filter to remove rare collocations\n",
    "    finder.apply_freq_filter(3)  # Adjust the frequency threshold as needed\n",
    "\n",
    "    # Extract the top collocations\n",
    "    top_collocations = finder.nbest(BigramAssocMeasures.likelihood_ratio, 10)  # Adjust the number of top collocations as needed\n",
    "\n",
    "    print(f\"Top Collocations for {label[i]}\")\n",
    "    for collocation in top_collocations:\n",
    "        print(collocation)\n",
    "    print('\\n\\n')\n",
    "    \n",
    "    # Specify the value of N for N-grams\n",
    "    n = 3  # Change this to the desired value of N\n",
    "\n",
    "    # Extract N-grams from the tokenized sentences\n",
    "    n_grams = [list(ngrams(sentence, n)) for sentence in tokenized_sentences]\n",
    "\n",
    "    # Flatten the list of N-grams\n",
    "    flat_n_grams = [gram for sublist in n_grams for gram in sublist]\n",
    "\n",
    "    # Count the frequency of each N-gram\n",
    "    n_gram_freq = nltk.FreqDist(flat_n_grams)\n",
    "\n",
    "    # Print the most common N-grams\n",
    "    print(f\"Most common {n}-grams for {labels[i]}:\")\n",
    "    for n_gram, freq in n_gram_freq.most_common(10):  # Adjust the number of most common N-grams as needed\n",
    "        print(f\"{n_gram}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mykernel",
   "language": "python",
   "name": "mykernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
